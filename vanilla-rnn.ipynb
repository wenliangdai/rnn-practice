{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This code is a re-produce of min-char-rnn.py (https://gist.github.com/karpathy/d4dee566867f8291f086)\n",
    "# Most parts of code are the same, with cleaner format and code style. (Python3)\n",
    "# For learning purposes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 5 characters, 4 unique.\n"
     ]
    }
   ],
   "source": [
    "# Data I/O\n",
    "data = open('text.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size = len(data)\n",
    "vocab_size = len(chars)\n",
    "print('data has {} characters, {} unique.'.format(data_size, vocab_size))\n",
    "char_to_ix = { ch: i for i, ch in enumerate(chars) }\n",
    "ix_to_char = { i: ch for i, ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "hidden_size = 100\n",
    "seq_length = 4\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "W_xh = np.random.randn(hidden_size, vocab_size) * 0.01 # input to hidden\n",
    "W_hh = np.random.randn(hidden_size, hidden_size) * 0.01 # hidden to hidden\n",
    "W_hy = np.random.randn(vocab_size, hidden_size) * 0.01 # hidden to output\n",
    "b_h = np.zeros((hidden_size, 1)) # hidden bias\n",
    "b_y = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "    \n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    \n",
    "    # forward pass\n",
    "    for t in np.arange(len(inputs)):\n",
    "        # encode input in 1-of-k representation\n",
    "        xs[t] = np.zeros((vocab_size, 1))\n",
    "        xs[t][inputs[t]] = 1\n",
    "        # hidden state\n",
    "        hs[t] = np.tanh(W_xh@xs[t] + W_hh@hs[t-1] + b_h)\n",
    "        # output (unnormalised potentials)\n",
    "        ys[t] = W_hy@hs[t] + b_y\n",
    "        # normalised probablities\n",
    "        ps[t] = np.exp(ys[t])/np.sum(np.exp(ys[t]))\n",
    "        # softmax (cross-entropy loss)\n",
    "        loss += -np.log(ps[t][targets[t]][0])\n",
    "\n",
    "    # backward pass\n",
    "    dLdW_xh, dLdW_hh, dLdW_hy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
    "    dLdb_h, dLdb_y = np.zeros_like(b_h), np.zeros_like(b_y)\n",
    "    dLdh_next = np.zeros_like(hs[0])\n",
    "    for t in np.flip(np.arange(len(inputs)),0):\n",
    "        # dL/dy\n",
    "        dLdy = np.copy(ps[t])\n",
    "        dLdy[targets[t]] -= 1\n",
    "        # dL/dW_hy\n",
    "        dLdW_hy += (dLdy@hs[t].T) # (v x 1) x (h x 1).T = (v x h)\n",
    "        # dL/db_y\n",
    "        dLdb_y += dLdy\n",
    "        # dL/dh\n",
    "        dLdh = W_hy.T@dLdy # (v x h).T x (v x 1) = (h x 1)\n",
    "        # backprop through tanh\n",
    "        dLdh_raw = (1 - hs[t]*hs[t]) * dLdh\n",
    "        # dL/db_h\n",
    "        dLdb_h += dLdh_raw\n",
    "        # dL/dW_hh\n",
    "        dLdW_hh = dLdh_raw@hs[t-1].T\n",
    "        # dL/dW_xh\n",
    "        dLdW_xh = dLdh_raw@xs[t].T\n",
    "        # hidden gradient flow to the previous step h[t-1]\n",
    "        dLdh_next = W_hh.T@dLdh_raw\n",
    "    \n",
    "    # clip to mitigate gradient vanish/explode\n",
    "    for dparam in [dLdW_hy, dLdW_hh, dLdW_xh, dLdb_y, dLdb_h]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "    \n",
    "    return loss, dLdW_xh, dLdW_hh, dLdW_hy, dLdb_h, dLdb_y, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(h, seed_index, n):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_index] = 1\n",
    "    indexes = []\n",
    "    for t in np.arange(n):\n",
    "        h = np.tanh(W_hh@h + W_xh@x + b_h)\n",
    "        y = W_hy@h + b_y\n",
    "        p = np.exp(y)/np.sum(np.exp(y))\n",
    "        index = np.random.choice(np.arange(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[index] = 1\n",
    "        indexes.append(index)\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " ooh \n",
      "----\n",
      "iter: 0, loss: 5.545175257662081\n",
      "----\n",
      " eol \n",
      "----\n",
      "iter: 100, loss: 5.186246652071687\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 200, loss: 4.837003404538811\n",
      "----\n",
      " elo \n",
      "----\n",
      "iter: 300, loss: 4.51425415542281\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 400, loss: 4.218479353670771\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 500, loss: 3.9482313233142747\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 600, loss: 3.7016064136498157\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 700, loss: 3.4766517838491615\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 800, loss: 3.2714943634228746\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 900, loss: 3.084378766634607\n",
      "----\n",
      " elo \n",
      "----\n",
      "iter: 1000, loss: 2.913680410457445\n",
      "----\n",
      " elo \n",
      "----\n",
      "iter: 1100, loss: 2.757908254243754\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 1200, loss: 2.615700487151063\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 1300, loss: 2.485816296827693\n",
      "----\n",
      " elo \n",
      "----\n",
      "iter: 1400, loss: 2.3671264902998224\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 1500, loss: 2.2586042801945174\n",
      "----\n",
      " elo \n",
      "----\n",
      "iter: 1600, loss: 2.1593165995639603\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 1700, loss: 2.068415996653596\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 1800, loss: 1.9851331027864136\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 1900, loss: 1.9087696610863478\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 2000, loss: 1.838692100505892\n",
      "----\n",
      " elo \n",
      "----\n",
      "iter: 2100, loss: 1.7743256346792442\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 2200, loss: 1.7151488605735596\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 2300, loss: 1.660688828377015\n",
      "----\n",
      " elo \n",
      "----\n",
      "iter: 2400, loss: 1.6105165514620101\n",
      "----\n",
      " elo \n",
      "----\n",
      "iter: 2500, loss: 1.5642429236183795\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 2600, loss: 1.521515010218864\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 2700, loss: 1.4820126806295448\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 2800, loss: 1.4454455508687414\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 2900, loss: 1.4115502079108566\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 3000, loss: 1.3800876897158618\n",
      "----\n",
      " elo \n",
      "----\n",
      "iter: 3100, loss: 1.3508411976927388\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 3200, loss: 1.323614020667455\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 3300, loss: 1.2982276514551294\n",
      "----\n",
      " elo \n",
      "----\n",
      "iter: 3400, loss: 1.2745200788590474\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 3500, loss: 1.252344239404504\n",
      "----\n",
      " elo \n",
      "----\n",
      "iter: 3600, loss: 1.2315666144314203\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 3700, loss: 1.2120659593666152\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 3800, loss: 1.1937321531022318\n",
      "----\n",
      " elo \n",
      "----\n",
      "iter: 3900, loss: 1.176465156432486\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 4000, loss: 1.1601740694487903\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 4100, loss: 1.144776278663477\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 4200, loss: 1.130196685426407\n",
      "----\n",
      " elo \n",
      "----\n",
      "iter: 4300, loss: 1.1163670079223027\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 4400, loss: 1.1032251496974315\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 4500, loss: 1.0907146282726814\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 4600, loss: 1.0787840579652586\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 4700, loss: 1.0673866815710935\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 4800, loss: 1.0564799460597294\n",
      "----\n",
      " ell \n",
      "----\n",
      "iter: 4900, loss: 1.0460251179048452\n"
     ]
    }
   ],
   "source": [
    "iterations = 5000\n",
    "pointer = 0 # data pointer\n",
    "mW_xh, mW_hh, mW_hy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
    "mb_h, mb_y = np.zeros_like(b_h), np.zeros_like(b_y) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size) * seq_length # loss at iteration 0\n",
    "\n",
    "for i in range(iterations):\n",
    "    if pointer+seq_length+1 > len(data) or i == 0:\n",
    "        hprev = np.zeros((hidden_size, 1)) # reset rnn memory\n",
    "        pointer = 0\n",
    "    inputs = [char_to_ix[ch] for ch in data[pointer:pointer+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[pointer+1:pointer+seq_length+1]]\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        sample_indexes = sample(hprev, inputs[0], 3)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_indexes)\n",
    "        print('----\\n {} \\n----'.format(txt))\n",
    "\n",
    "    loss, dLdW_xh, dLdW_hh, dLdW_hy, dLdb_h, dLdb_y, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print('iter: {}, loss: {}'.format(i, smooth_loss))\n",
    "        \n",
    "    for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y], \n",
    "                                [dLdW_xh, dLdW_hh, dLdW_hy, dLdb_h, dLdb_y], \n",
    "                                [mW_xh, mW_hh, mW_hy, mb_h, mb_y]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "    \n",
    "    pointer += seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
